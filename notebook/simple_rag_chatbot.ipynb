{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot that Doesn't Suck\n",
    "\n",
    "In this notebook we'll build a RAG-based chatbot for a small furniture manufacturer in Oahu, Hawaii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iShApPi-09Xj"
   },
   "source": [
    "## Set auth tokens\n",
    "\n",
    "In this notebook we'll use:\n",
    "\n",
    "- [Jina Embeddings v2]()\n",
    "- [Hugging Face Inference API]()\n",
    "\n",
    "You'll need to get tokens for each of the above and enter them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2agYA5_TyX5",
    "outputId": "19443e35-12af-4f27-b14b-346e27a37761"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your Jina Embeddings API key:  ········\n",
      "Your Hugging Face Inference API key:  ········\n",
      "Ngrok auth token:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "jinaai_api_key = getpass(prompt=\"Your Jina Embeddings API key: \")\n",
    "hf_inference_api_key = getpass(prompt=\"Your Hugging Face Inference API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSvL4dpfUJur",
    "outputId": "188f1d5f-7618-4693-9a74-e44103b8ff6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# RAG dependencies\n",
    "!pip install -q llama-index llama-index-llms-openai llama-index-embeddings-jinaai llama-index-llms-huggingface \"huggingface_hub[inference]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUMsMe3TTTNu"
   },
   "source": [
    "## Process data\n",
    "\n",
    "We used GPT to generate some sample data for a fictitious small furniture maker in Oahu, Hawaii. This consists of four simple HTML pages:\n",
    "\n",
    "- FAQ page\n",
    "- Front page\n",
    "- Contact page\n",
    "- Product listings page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tuHsDJ3nLLwP"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SktkIQxcLpi4"
   },
   "outputs": [],
   "source": [
    "# cleanup from last run\n",
    "!rm -rf data\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_MjeH2ViMHpQ"
   },
   "outputs": [],
   "source": [
    "# download html files\n",
    "!wget -q https://github.com/alexcg1/rag-chatbot/raw/main/notebook/data/faq.html --directory-prefix data/\n",
    "!wget -q https://github.com/alexcg1/rag-chatbot/raw/main/notebook/data/front.html --directory-prefix data/\n",
    "!wget -q https://github.com/alexcg1/rag-chatbot/raw/main/notebook/data/contact.html --directory-prefix data/\n",
    "!wget -q https://github.com/alexcg1/rag-chatbot/raw/main/notebook/data/products.html --directory-prefix data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FFlCKDQNLSEi"
   },
   "outputs": [],
   "source": [
    "# store html files in list\n",
    "data_dir = \"./data\"\n",
    "html_files = glob(f'{data_dir}/*.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Markdown\n",
    "\n",
    "HTML is a pain to break into chunks and unreliable for LLMs to parse. We'll convert it to [markdown]() to make things easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "2X-IgrP6Nz1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandoc: ./data/front.html: withBinaryFile: does not exist (No such file or directory)\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['pandoc', '--markdown-headings=atx', './data/front.html', '-o', './data/front.md']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;66;03m# colab pandoc\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\n\u001b[1;32m     10\u001b[0m   subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandoc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--atx-headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-o\u001b[39m\u001b[38;5;124m\"\u001b[39m, md_file], check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandoc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--atx-headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-o\u001b[39m\u001b[38;5;124m\"\u001b[39m, md_file], check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# newer pandoc\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpandoc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--markdown-headings=atx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd_file\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m md_files \u001b[38;5;241m=\u001b[39m glob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*.md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.12/subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    572\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['pandoc', '--markdown-headings=atx', './data/front.html', '-o', './data/front.md']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# convert html files to markdown for easier chunking\n",
    "for filename in html_files:\n",
    "  base_name = os.path.splitext(filename)[0]\n",
    "  md_file = os.path.join(base_name + \".md\")\n",
    "\n",
    "  # Colab uses ancient pandoc, with different argument for markdown header style\n",
    "  try:\n",
    "    # colab pandoc\n",
    "    subprocess.run([\"pandoc\", \"--atx-headers\", filename, \"-o\", md_file], check=True)\n",
    "  except:\n",
    "    # newer pandoc\n",
    "    subprocess.run([\"pandoc\", \"--markdown-headings=atx\", filename, \"-o\", md_file], check=True)\n",
    "\n",
    "md_files = glob(f'{data_dir}/*.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break Pages into Chunks\n",
    "\n",
    "We'll make the data more digestible to our chatbot by breaking it into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wkpAyY4rQkaI"
   },
   "outputs": [],
   "source": [
    "# break markdown files into chunks\n",
    "docs = []\n",
    "\n",
    "for md_file in md_files:\n",
    "  with open(md_file, 'r') as f:\n",
    "    content = f.read()\n",
    "    docs.extend(content) # add full page\n",
    "\n",
    "    content_chunks = content.split(\"\\n#\")\n",
    "    docs.extend(content_chunks) # add individual section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_GVRPqVTw5T"
   },
   "source": [
    "## Build RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZZ1ZtLtYel3"
   },
   "source": [
    "### Access Jina Embeddings v2 via the LlamaIndex interface.\n",
    "\n",
    "This code creates the LlamaIndex object that manages your connection to the Jina Embeddings v2 API.\n",
    "\n",
    "The resulting object is held in the variable `jina_embedding_model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "itaj6kwwUvki"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "\n",
    "jina_embedding_model = JinaEmbedding(\n",
    "    api_key=jinaai_api_key,\n",
    "    model=\"jina-embeddings-v2-base-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJynvz-VYaPw"
   },
   "source": [
    "### Access the Mixtral Model via the HuggingFace Inference API\n",
    "\n",
    "This code creates a holder for accessing the `mistralai/Mixtral-8x7B-Instruct-v0.1` model via the Hugging Face Inference API. The resulting object is held in the variable `mixtral_llm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwElcMppVWqj",
    "outputId": "61999353-d80a-4d8f-c179-6e995f888a85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexcg/work/repos/jina-alexcg/blog/chatsmith/rag-chatbot/notebook/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alexcg/work/repos/jina-alexcg/blog/chatsmith/rag-chatbot/notebook/env/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1237768/4156006060.py:3: DeprecationWarning: Call to deprecated class HuggingFaceInferenceAPI. (Deprecated in favor of `HuggingFaceInferenceAPI` from `llama-index-llms-huggingface-api` which should be used instead.)\n",
      "  mixtral_llm = HuggingFaceInferenceAPI(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "mixtral_llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", token=hf_inference_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUJfrfJZasMS"
   },
   "source": [
    "### Convert chunks to be suitable for LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WyH3fEHdauze"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.readers import StringIterableReader\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "chunks = StringIterableReader().load_data(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoCxdn7BYkD_"
   },
   "source": [
    "### Create a Service\n",
    "\n",
    "The code creates a RAG service that has access to Jina Embeddings and Mixtral Instruct and stores it in the variable `service_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5zL2rmyYmes",
    "outputId": "436bbe5e-af7b-4128-fd2b-e8e61637c25c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1237768/3144623853.py:3: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=mixtral_llm, embed_model=jina_embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJYsOxGDYqHV"
   },
   "source": [
    "### Build the document index\n",
    "\n",
    "Next, we store the documents in LlamaIndex' `VectorStoreIndex`, generating embeddings with Jina Embeddings v2 model and using them as keys for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NQcAN4lIVaD8"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=chunks, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Att4O7CuYxOl"
   },
   "source": [
    "### Prepare a Prompt Template\n",
    "\n",
    "This is the prompt template that will be presented to Mixtral Instruct, with `{context_str}` and `{query_str}` replaced with the retrieved documents and your query respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RYXfmEriYxiB"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query. Please be brief, concise, and complete.\\n\"\n",
    "    \"If the context information does not contain an answer to the query, \"\n",
    "    \"respond with \\\"I'm sorry, but we don't have any information about that. Please contact us on info@oahufurniture.com for more information.\\\".\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ip0RS-LZDfY"
   },
   "source": [
    "### Assemble the Full Query Engine\n",
    "\n",
    "The query engine has three parts:\n",
    "\n",
    "* `retriever` is the search engine that takes user requests and retrieves relevant documents from the vector store.\n",
    "* `response_synthesizer` uses the prompt created above to join the retrieved documents and user request and passes them to the LLM, getting back its response.\n",
    "* `query_engine` is a container object that holds the two together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t-5SMeo8YznS"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    text_qa_template=qa_prompt,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbBmF1vObopZ"
   },
   "source": [
    "## Run some queries\n",
    "\n",
    "Let's run some queries to see our chatbot in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "    result = query_engine.query(question)\n",
    "    return result.response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A computer can be useful on a farm for various tasks such as managing financial records, tracking crop yields, monitoring weather patterns, and accessing online resources for farming tips and techniques. It can also be used for communication and collaboration purposes, such as coordinating with other farmers or suppliers. Additionally, computers can be used to operate and monitor automated farming equipment, making farm operations more efficient and precise.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(\"How is a computer useful on a farm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3E9lsW5bq3b",
    "outputId": "32e86ceb-16fd-448d-be04-bfae4e256679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We make sustainable furniture using locally sourced native timbers that are harvested responsibly. We can tailor designs to match specific themes or decor styles for residential and commercial spaces.\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"What kind of furniture do you make?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO-W7AYjcf0V",
    "outputId": "1b18cda7-95a6-432f-8028-58e9d096d915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm sorry, but we don't have any information about that. Please contact us on info@oahufurniture.com for more information.\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"How much does your furniture cost?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXYodN8aco-5",
    "outputId": "bb9fdcd2-b15d-4d16-c60d-4d55af79f9b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We do not have a public showroom, but we can arrange viewings of specific furniture pieces by appointment at our workshop in Honolulu.\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"Can I see your furniture in person?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZ11omuWdAhF",
    "outputId": "5438b069-5da0-4a21-ef42-add80386b607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We accept major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers.\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"What payment methods do you accept?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wHdY2_MrdMV1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our furniture is made from locally sourced native timbers such as Koa, Milo, and Kamani.\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"What is your furniture made from?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing in different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Quw9FaH7dUSS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们的家具是用高质量的木材制成的。\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"你的家具是用什么材料制成的？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WyN7ue3nddSr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Die Zahlungsmethoden, die von Oahu Furniture akzeptiert werden, sind Kreditkarten (Visa, Mastercard, American Express), PayPal, Apple Pay, Google Pay und Banküberweisung.\n"
     ]
    }
   ],
   "source": [
    "get_answer(\"Welche Zahlungsmethoden werden akzeptiert?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your own queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Please enter your question: \")\n",
    "    answer = get_answer(question)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVnt2ya0mwk6"
   },
   "source": [
    "## Set up API for external access\n",
    "\n",
    "If you're running this locally in a Jupyter notebook (i.e. *not* Google Colab) you can test the chatbot via a RESTful API and simple web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "z4dK1B56lOG1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q fastapi uvicorn requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * ngrok tunnel \"https://0c6b-212-20-115-56.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1237768]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:38934 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:60334 - \"OPTIONS / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:50398 - \"OPTIONS / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:50398 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     212.20.115.56:0 - \"OPTIONS / HTTP/1.1\" 200 OK\n",
      "INFO:     212.20.115.56:0 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     212.20.115.56:0 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     212.20.115.56:0 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:38850 - \"OPTIONS / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:38850 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:49656 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:45126 - \"OPTIONS / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:45134 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:32806 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54160 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:34126 - \"POST /shutdown HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (stop_uvicorn):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexcg/work/repos/jina-alexcg/blog/chatsmith/rag-chatbot/notebook/env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1237768/870245213.py\", line 48, in stop_uvicorn\n",
      "NameError: name 'uvicorn_server' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59876 - \"OPTIONS / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59890 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:47492 - \"POST /shutdown HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9 (stop_uvicorn):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexcg/work/repos/jina-alexcg/blog/chatsmith/rag-chatbot/notebook/env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1237768/870245213.py\", line 48, in stop_uvicorn\n",
      "NameError: name 'uvicorn_server' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59914 - \"POST /shutdown HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10 (stop_uvicorn):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexcg/work/repos/jina-alexcg/blog/chatsmith/rag-chatbot/notebook/env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1237768/870245213.py\", line 48, in stop_uvicorn\n",
      "NameError: name 'uvicorn_server' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:46346 - \"POST /shutdown HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11 (stop_uvicorn):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexcg/work/repos/jina-alexcg/blog/chatsmith/rag-chatbot/notebook/env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1237768/870245213.py\", line 48, in stop_uvicorn\n",
      "NameError: name 'uvicorn_server' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:43196 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53152 - \"POST /shutdown HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12 (stop_uvicorn):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/alexcg/work/repos/jina-alexcg/blog/chatsmith/rag-chatbot/notebook/env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1237768/870245213.py\", line 48, in stop_uvicorn\n",
      "NameError: name 'uvicorn_server' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:53162 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import threading\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "conf.get_default().auth_token = ngrok_token\n",
    "\n",
    "# Enable CORS\n",
    "origins = [\"*\"] # all origins\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,  # Allows all origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # Allows all methods\n",
    "    allow_headers=[\"*\"],  # Allows all headers\n",
    ")\n",
    "# Enable cors end code\n",
    "\n",
    "# Open a ngrok tunnel to the HTTP server\n",
    "public_url = ngrok.connect(port).public_url\n",
    "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
    "\n",
    "# Update any base URLs to use the public ngrok URL\n",
    "app.state.BASE_URL = public_url\n",
    "\n",
    "# Define FastAPI routes\n",
    "\n",
    "@app.post('/')\n",
    "async def chat_endpoint(request: Request):\n",
    "    data = await request.json()  # Get JSON data from the request\n",
    "    response_data = {\n",
    "        \"question\": data[\"question\"],\n",
    "        \"answer\": get_answer(data[\"question\"])\n",
    "    }\n",
    "    \n",
    "    return response_data\n",
    "\n",
    "@app.post('/shutdown')\n",
    "async def shutdown():\n",
    "    global server_running\n",
    "    server_running = False\n",
    "    def stop_uvicorn():\n",
    "        uvicorn_server.should_exit = True\n",
    "    threading.Thread(target=stop_uvicorn).start()\n",
    "    return {\"message\": \"Server shutting down...\"}\n",
    "\n",
    "# Start the FastAPI server in a new thread\n",
    "def run():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "threading.Thread(target=run).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in browser\n",
    "\n",
    "We can open a simple HTML chatbot page for you to test out the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rag-chatbot'...\n",
      "remote: Enumerating objects: 18, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 18 (delta 1), reused 18 (delta 1), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (18/18), 9.08 KiB | 9.08 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/alexcg1/rag-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"./rag-chatbot/web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving at port 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jul/2024 15:13:52] \"GET / HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [09/Jul/2024 15:13:52] \"GET /styles.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [09/Jul/2024 15:13:52] \"GET /script.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [09/Jul/2024 15:14:31] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2024 15:14:31] \"GET /styles.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2024 15:14:31] \"GET /script.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [09/Jul/2024 15:14:31] code 404, message File not found\n",
      "127.0.0.1 - - [09/Jul/2024 15:14:31] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m socketserver\u001b[38;5;241m.\u001b[39mTCPServer((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, web_port), handler) \u001b[38;5;28;01mas\u001b[39;00m httpd:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServing at port\u001b[39m\u001b[38;5;124m\"\u001b[39m, web_port)\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mhttpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve_forever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/socketserver.py:235\u001b[0m, in \u001b[0;36mBaseServer.serve_forever\u001b[0;34m(self, poll_interval)\u001b[0m\n\u001b[1;32m    232\u001b[0m selector\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mself\u001b[39m, selectors\u001b[38;5;241m.\u001b[39mEVENT_READ)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__shutdown_request:\n\u001b[0;32m--> 235\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# bpo-35017: shutdown() called during select(), exit immediately.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__shutdown_request:\n",
      "File \u001b[0;32m/usr/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import http.server\n",
    "import socketserver\n",
    "import os\n",
    "\n",
    "web_port = 8000\n",
    "\n",
    "handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "with socketserver.TCPServer((\"\", web_port), handler) as httpd:\n",
    "    print(\"Serving at port\", web_port)\n",
    "    httpd.serve_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can open your web browser to [http://localhost:8000](http://localhost:8000) to play with the chatbot in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop server when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def stop_server():\n",
    "    response = requests.post(f\"http://localhost:{port}/shutdown\")\n",
    "    print(response.content)\n",
    "    ngrok.disconnect(public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"message\":\"Server shutting down...\"}'\n"
     ]
    }
   ],
   "source": [
    "stop_server()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
